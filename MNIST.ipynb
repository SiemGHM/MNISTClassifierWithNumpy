{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.zeros((10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoding(y, num_classes=10):\n",
    "    # one_hot = np.zeros((num_classes))\n",
    "    # one_hot[y] = 1\n",
    "    # return one_hot\n",
    "    \n",
    "    one_hot = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    \n",
    "    return one_hot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('digit-recognizer/train.csv')\n",
    "data.head()\n",
    "y = data['label'].values\n",
    "X = data.drop('label', axis=1).values\n",
    "X = np.array(X).T\n",
    "y = np.array(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DerivRelu(x):\n",
    "        return np.where(x > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:, :41000], X[:, 41000:]\n",
    "y_train, y_test = y[:41000], y[41000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def Softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, iter, lam):\n",
    "\n",
    "        self.iter = iter\n",
    "        self.lam = lam\n",
    "        self.W1 = np.random.randn(64, 784) * np.sqrt(2. / 784)\n",
    "        self.b1 = np.zeros((64, 1))\n",
    "        self.W2 = np.random.randn(10, 64) * np.sqrt(2. / 64)\n",
    "        self.b2 = np.zeros((10, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        self.Z1 = np.dot(self.W1, self.X) + self.b1\n",
    "        self.A1 = Relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = Softmax(self.Z2)\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def backprop(self):\n",
    "        \n",
    "        self.onehoty = OneHotEncoding(self.y)\n",
    "        \n",
    "        m = self.y.size\n",
    "        dZ2 = self.A2 - self.onehoty \n",
    "        dW2 = 1/m * np.dot(dZ2, self.A1.T )\n",
    "        db2 = 1/m * np.sum(dZ2, 1)\n",
    "        dZ1 = self.W2.T.dot(dZ2) * DerivRelu(self.Z1)\n",
    "        dW1 = 1/m * np.dot(dZ1, self.X.T)\n",
    "        db1 = 1/m * np.sum(dZ1, 1)\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        \n",
    "        # print(dW1.shape, self.W1.shape)\n",
    "        self.W1 -= self.lam * dW1\n",
    "        # print(\" shapes b1, update \", self.b1.shape, db1.shape)\n",
    "        self.b1 -= self.lam * db1.reshape(-1,1)   \n",
    "        self.W2 -= self.lam * dW2\n",
    "        self.b2 -= self.lam * db2.reshape(-1,1)  \n",
    "        \n",
    "        \n",
    "         \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            self.forward()\n",
    "            self.backprop()\n",
    "            # self.lossfunc()\n",
    "            # print(self.loss)\n",
    "            if i%20==0:\n",
    "                l = self.getAccuracy(self.predict(X), y)\n",
    "                print(f'Iteration {i} loss: {l}')\n",
    "                print(f'Iteration {i} loss: {self.cross_entropy_loss(self.y, self.A2)}')\n",
    "                pass \n",
    "            \n",
    "            \n",
    "                   \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        # Ensure numerical stability with clipping\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # Compute the loss\n",
    "        divisor = y_true.shape[1] if y_true.ndim > 1 else y_true.size\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / divisor\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.forward()\n",
    "        return np.argmax(self.A2, axis=0)\n",
    "    \n",
    "    # def loss(self):\n",
    "    #     self.lossvalue = -np.sum(self.y * np.log(self.A2)) / self.X.shape[1] + self.lam * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2))) / (2 * self.X.shape[1])  \n",
    "    #     print(self.lossvalue)\n",
    "    #     return self.lossvalue\n",
    "    \n",
    "    \n",
    "\n",
    "    def getAccuracy(self, predictions, y):\n",
    "        return np.sum(predictions == y) / y.size  \n",
    "        \n",
    "    # def lossfunc(self):\n",
    "    #     self.loss = -np.sum(self.y * np.log(self.A2)) / self.X.shape[1] + self.lam * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2))) / (2 * self.X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 41000)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotEncoding(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 0.29821951219512194\n",
      "Iteration 0 loss: 124.2011152076999\n",
      "Iteration 20 loss: 0.8553658536585366\n",
      "Iteration 20 loss: 217.65862834793364\n",
      "Iteration 40 loss: 0.8898048780487805\n",
      "Iteration 40 loss: 255.68747004898697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[297], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m (X_train \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m OneHotEncoding(y_train)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[295], line 53\u001b[0m, in \u001b[0;36mNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward()\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# self.lossfunc()\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# print(self.loss)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[295], line 28\u001b[0m, in \u001b[0;36mNN.backprop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehoty \u001b[38;5;241m=\u001b[39m OneHotEncoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     27\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m---> 28\u001b[0m dZ2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monehoty\u001b[49m \n\u001b[1;32m     29\u001b[0m dW2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dZ2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA1\u001b[38;5;241m.\u001b[39mT )\n\u001b[1;32m     30\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ2, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NN(500, 0.1)\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "\n",
    "OneHotEncoding(y_train).shape\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
